{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLAES Units Preprocessing\n",
    "\n",
    "This notebook contains code for preprocessing single unit data collected during the BLAES encoding experiments. The raw 30 kHz data is loaded from the `.ns6`/`.nev` or `.mat` files, and subsequently restructured as `pd.DataFrames`. Basic data features are summarized (e.g., # units) and exported as `.csv` or `.txt` files. Stimulation pulses are detected from the NSP Sync channel, and used to define peri-stimulation epochs (± 1) for analysis. Plotting functions enable visualization of individual unit waveforms, a raster plot for the entire recording session, and peri-stimulation raster plots (within-channel).\n",
    "\n",
    "---\n",
    "\n",
    "> *Author: Justin Campbell (justin.campbell@hsc.utah.edu)*  \n",
    "> *Version: 06/24/2024*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Notebook\n",
    "Import libraries, utilities, and packages. Configure notebook figure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import mat73\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import find_peaks, butter, filtfilt, find_peaks, correlate, decimate\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "# Import Blackrock Python Utilities\n",
    "sys.path.append('/Users/justincampbell/Library/CloudStorage/GoogleDrive-u0815766@gcloud.utah.edu/My Drive/Research Projects/BLAESUnits/Code/Blacrock-Python-Utilities')\n",
    "import brpylib\n",
    "\n",
    "%matplotlib inline\n",
    "# %config InlineBackend.figure_format='retina'\n",
    "%config InlineBackend.figure_format='svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNEV(nev_path):\n",
    "    '''\n",
    "    Load .nev file and extract spike events and waveforms.\n",
    "    \n",
    "    Arguments:\n",
    "        nev_path (str): path to .nev file\n",
    "    Returns:\n",
    "        events (pd.DataFrame): spike events\n",
    "        waveforms (np.ndarray): spike waveforms\n",
    "    '''\n",
    "    \n",
    "    # Open .nev file\n",
    "    nev_file = brpylib.NevFile(nev_path)\n",
    "    \n",
    "    # Extract data\n",
    "    print('\\nLoading .nev data...')\n",
    "    nev_data = nev_file.getdata()['spike_events']\n",
    "    \n",
    "    # Close .nev file\n",
    "    nev_file.close()\n",
    "    \n",
    "    # Extract waveforms\n",
    "    waveforms = nev_data['Waveforms']\n",
    "    \n",
    "    # Convert spike events to pd.DataFrame\n",
    "    spikes = nev_data.copy()\n",
    "    spikes.pop('Waveforms', None)\n",
    "    events = pd.DataFrame.from_dict(spikes)\n",
    "    \n",
    "    # Define valid unit labels\n",
    "    valid_unit_labels = np.arange(1,17) # 0 = unclassified, 255 = noise, 1-16 = valid units\n",
    "    events = events[events['Unit'].isin(valid_unit_labels)]\n",
    "    valid_unit_idxs = events.index.values\n",
    "    waveforms = waveforms[valid_unit_idxs]\n",
    "    \n",
    "    # Reindex\n",
    "    events.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return events, waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNSX(nsx_path):\n",
    "    '''\n",
    "    Load .nsx file and extract data and header.\n",
    "    \n",
    "    Arguments:\n",
    "        nsx_path (str): path to .nsx file\n",
    "        \n",
    "    Returns:\n",
    "        nsx_data (np.ndarray): raw neural data (30 kHz)\n",
    "        header (dict): header (recording info)\n",
    "    '''\n",
    "    # Open .nsx file\n",
    "    nsx_file = brpylib.NsxFile(nsx)\n",
    "\n",
    "    # Extract data & header\n",
    "    print('\\nLoading .nsx data...')\n",
    "    nsx_data = nsx_file.getdata(full_timestamps = True)['data'][0]\n",
    "    header = nsx_file.extended_headers\n",
    "\n",
    "    # Close .nev file\n",
    "    nsx_file.close()\n",
    "    \n",
    "    return nsx_data, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSortedMat(mat_path):\n",
    "    '''\n",
    "    Load sorted .mat file and extract data and chan_labels.\n",
    "    \n",
    "    Arguments:\n",
    "        mat_path (str): path to *sorted.mat file\n",
    "        \n",
    "    Returns:\n",
    "        mat_dict (dict): micro channels with sorted units\n",
    "        chan_labels (list): micro channel labels (all channels)\n",
    "    '''\n",
    "    # load the mat file\n",
    "    mat_dict = loadmat(mat_path, simplify_cells=True)\n",
    "\n",
    "    # load WashU unit labels\n",
    "    washU_labels = pd.read_csv('/Users/justincampbell/Library/CloudStorage/GoogleDrive-u0815766@gcloud.utah.edu/My Drive/Research Projects/BLAESUnits/WashU Micro Labels.csv')\n",
    "    lead_labels = washU_labels[washU_labels['pID'] == pID[:-2]]['Label'].values\n",
    "\n",
    "    # create a list of all channels\n",
    "    chan_labels = []\n",
    "    for i in range(len(lead_labels)):\n",
    "        for ii in range(1, 9):\n",
    "            chan_labels.append(lead_labels[i] + str(ii))\n",
    "\n",
    "    # remove variables with '__' in them\n",
    "    mat_dict = {k: v for k, v in mat_dict.items() if '__' not in k}\n",
    "\n",
    "    # remove Sync channel\n",
    "    if len(mat_dict.keys()) == 17:\n",
    "        mat_dict.pop('Channel17', None)\n",
    "    elif len(mat_dict.keys()) == 9:\n",
    "        mat_dict.pop('Channel09', None)\n",
    "\n",
    "    # rename keys with channel names\n",
    "    mat_dict = {chan_labels[i]: v for i, (k, v) in enumerate(mat_dict.items())}\n",
    "\n",
    "    # remove channels that are empty (no units)\n",
    "    mat_dict = {k: v for k, v in mat_dict.items() if v.size > 0}\n",
    "    \n",
    "    return mat_dict, chan_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_path = '/Volumes/Hippocampus/BLAESUnits/Data_30k'\n",
    "\n",
    "# Find relevant .nev, .nsx, and .mat files\n",
    "nev_files = glob.glob(root_dir = data_path, pathname = '**/*.nev', recursive = True)\n",
    "nsx_files = glob.glob(root_dir = data_path, pathname = '**/*.ns6', recursive = True)\n",
    "sorted_mat_files = glob.glob(root_dir = data_path, pathname = '**/*sorted.mat', recursive = True)\n",
    "raw_mat_files = glob.glob(root_dir = data_path, pathname = '**/*raw.mat', recursive = True)\n",
    "\n",
    "# Sort based on filename\n",
    "nev_files.sort()\n",
    "nsx_files.sort()\n",
    "sorted_mat_files.sort()\n",
    "raw_mat_files.sort()\n",
    "\n",
    "# Combine to create list of sessions\n",
    "sessions = [nev_files[i].split('/')[0] for i in range(len(nev_files))]\n",
    "for i in range(len(sorted_mat_files)):\n",
    "    sessions.append(sorted_mat_files[i].split('/')[0])\n",
    "\n",
    "# Data params\n",
    "fs = 30000 # sampling rate\n",
    "\n",
    "# Display\n",
    "print('Found data for %s sessions:' % (len(sessions)))\n",
    "print('----------------------------')\n",
    "for i in range(len(sessions)):\n",
    "    print('%s: %s' % (i, sessions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the session file to analyze\n",
    "file_idx = int(input('Enter the session number to analyze:'))\n",
    "fileName = sessions[file_idx].split('/')[-1]\n",
    "fType = fileName[0:3]\n",
    "\n",
    "# Print pID\n",
    "pID = fileName\n",
    "proj_path = '/Users/justincampbell/Library/CloudStorage/GoogleDrive-u0815766@gcloud.utah.edu/My Drive/Research Projects/BLAESUnits'\n",
    "save_path = os.path.join(proj_path, 'Results', pID)\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    print('Processing session %s...' % pID)\n",
    "else:\n",
    "    print('Session %s already processed. Overwriting...' % pID)\n",
    "\n",
    "# Utah files\n",
    "if fType == 'UIC':\n",
    "    # Locate files\n",
    "    nev = os.path.join(data_path, sessions[file_idx], sessions[file_idx] + '.nev')\n",
    "    nsx = os.path.join(data_path, sessions[file_idx], sessions[file_idx] + '.ns6')\n",
    "        \n",
    "    # Load .nev and .nsx files\n",
    "    events, waveforms = loadNEV(nev)\n",
    "    cont_data, header = loadNSX(nsx)\n",
    "\n",
    "    # Adjust for Blackrock's convention of multiplying uV by 4\n",
    "    waveforms = waveforms / 4\n",
    "\n",
    "    # Get chan IDs and chan labels\n",
    "    chan_IDs = [header[i]['ElectrodeID'] for i in range(len(header))]\n",
    "    chan_labels = [header[i]['ElectrodeLabel'] for i in range(len(header))]\n",
    "\n",
    "    # Create a dictionary to map chan_IDs to chan_labels\n",
    "    chan_map = {}\n",
    "    for idx, val in enumerate(chan_IDs):\n",
    "        chan_map[val] = chan_labels[idx] # adjust for Python indexing\n",
    "\n",
    "    # Map electrode labels to channel numbers\n",
    "    events['Channel'] = events['Channel'].map(chan_map)\n",
    "    \n",
    "    # Get sync & PD channels\n",
    "    sync = cont_data[chan_labels.index('Sync'),:]\n",
    "    PD = cont_data[chan_labels.index('PD'),:]\n",
    "    \n",
    "    # # Delete cont_data to save memory\n",
    "    # del cont_data\n",
    "\n",
    "\n",
    "# WashU files\n",
    "elif fType == 'BJH':\n",
    "    # Locate files\n",
    "    sorted_mat = os.path.join(data_path, sessions[file_idx], sessions[file_idx] + '_sorted.mat')\n",
    "    trigger_mat = os.path.join(data_path, sessions[file_idx], 'Stim_trigger.mat')\n",
    "    raw_mat = os.path.join(data_path, sessions[file_idx], sessions[file_idx] + '_raw.mat')\n",
    "    \n",
    "    # Load sorted.mat and raw.mat files\n",
    "    mat_dict, chan_labels = loadSortedMat(sorted_mat)\n",
    "    sync = loadmat(trigger_mat, simplify_cells = True)['stimTrigger']\n",
    "    try:\n",
    "        cont_data = loadmat(raw_mat, simplify_cells = True)['signals']\n",
    "    except:\n",
    "        cont_data = mat73.loadmat(raw_mat)['signals'] # if saved with v7.3 compression\n",
    "    \n",
    "    # Get events and waveforms\n",
    "    events = []\n",
    "    waveforms = []\n",
    "    for k, v in mat_dict.items():\n",
    "        df = pd.DataFrame(mat_dict[k])\n",
    "        df['Channel'] = [chan_labels[int(df[0][i]-1)] for i in range(df.shape[0])]\n",
    "        df['Unit'] = [int(df[1][i]) for i in range(df.shape[0])]\n",
    "        df['TimeStamps'] = [int(df[2][i]*fs) for i in range(df.shape[0])]\n",
    "        events.append(df[['TimeStamps', 'Unit', 'Channel']])\n",
    "        waveforms.append(df.iloc[:, 6:-3])\n",
    "    events = pd.concat(events)\n",
    "    waveforms = np.array(pd.concat(waveforms))\n",
    "    \n",
    "    # Adjust uV\n",
    "    waveforms = waveforms / 4\n",
    "    \n",
    "else:\n",
    "    print('Filetype not recognized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get micro lead (bundle) labels\n",
    "micro_chans = [x for x in chan_labels if x.startswith('m')]\n",
    "active_micro_chans = [x for x in events['Channel'].unique().tolist()]\n",
    "micro_leads = set([x[:-1] for x in micro_chans])\n",
    "active_micro_leads = set([x[:-1] for x in active_micro_chans])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export events and waveforms\n",
    "events.to_csv(os.path.join(save_path, 'Events.csv'))\n",
    "pd.DataFrame(waveforms).to_csv(os.path.join(save_path, 'Waveforms.csv')) # waveforms have different num samples (Blackrock: 48, Nihon Koden: 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summarize Dataset\n",
    "Generate summary statistics to characterize the dataset (e.g., # units detected).  \n",
    "Export features in `.csv` and `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizeData():\n",
    "    print('Session: %s' %pID)\n",
    "    print('- Recording Duration: %.1f min' %(len(sync) / fs / 60))\n",
    "    print('- Leads (Bundles) w/ Units: %s (%.0f%%)' %(len(active_micro_leads), (len(active_micro_leads) / len(micro_leads))*100))\n",
    "    print('- Channels w/ Units: %s (%.0f%%)' %(len(active_micro_chans), (len(active_micro_chans) / len(micro_chans))*100))\n",
    "    print('- Units Detected: %s' %str(summaryDF.shape[0]))\n",
    "    print('- Waveforms (Mean ± SD): %.1f ± %.1f' %(summaryDF['Waveforms'].mean(), summaryDF['Waveforms'].std()))\n",
    "    print('\\n')\n",
    "    print('Processed: ' + str(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group events by Channel and Unit, count the number of waveforms for each unit\n",
    "summaryDF = events.groupby(['Channel', 'Unit']).count()\n",
    "summaryDF.rename(columns={'TimeStamps': 'Waveforms'}, inplace=True)\n",
    "unit_labels = summaryDF.reset_index()['Channel'].astype(str) + '-' + summaryDF.reset_index()['Unit'].astype(str)\n",
    "\n",
    "# Display data summary\n",
    "summarizeData()\n",
    "summaryDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture cap\n",
    "summarizeData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save descriptives\n",
    "summaryDF.to_csv(os.path.join(save_path, 'WaveformCounts.csv'))\n",
    "with open(os.path.join(save_path, 'Summary.txt'), 'w') as f:\n",
    "    f.write(str(cap))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Epochs\n",
    "Epochs are created using the `sync` and `PD` channels within the recording to define the stim/no-stim trials, respectively. Because of the way in which the data were recorded, as well as noise/artifact present in the `sync`/`PD` channels, this section entails a considerable amount of manual tuning to ensure epochs are correctly aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Load Trial Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_info = pd.read_csv(os.path.join(proj_path, 'TrialInfo', pID + '_TrialInfo.csv'))\n",
    "trial_info[['Onset_SC', 'Onset_PD']] = trial_info[['Onset_SC', 'Onset_PD']].apply(lambda x: x * 15) # convert from 2->30 kHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Detect Sync Channel Onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSyncOnsets(sync):\n",
    "    '''\n",
    "    Detect sync pulses within the 30 kHz recording.\n",
    "    \n",
    "    Arguments:\n",
    "        sync (np.ndarray): sync channel data\n",
    "        \n",
    "    Returns:\n",
    "        stim_onsets (np.ndarray): array of stim_onset indices\n",
    "    '''\n",
    "\n",
    "    # Define threshold for sync channel\n",
    "    if fType == 'UIC':\n",
    "        thresh = 15000\n",
    "    elif fType == 'BJH':\n",
    "        thresh = 15000\n",
    "\n",
    "    # Find stim pulses using the following criteria: \n",
    "    # (1) sync > threshold, (2) transitioning from below to above threshold, (3) > 5s since last crossing\n",
    "    stim_onsets = []\n",
    "    syncBool = (sync > thresh).astype(int)\n",
    "    counter = 0\n",
    "    for i in range(len(syncBool)-1):\n",
    "        if (syncBool[i] == 1) & (syncBool[-i] == 0) & (counter > (fs*5)):\n",
    "            stim_onsets.append(i)\n",
    "            counter = 0\n",
    "        counter += 1\n",
    "    \n",
    "    return stim_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect sync (stim) pulses\n",
    "sync_onsets = getSyncOnsets(sync)\n",
    "\n",
    "# Add sync_onsets to trial_info\n",
    "special_cases = ['BJH02503', 'BJH02703', 'BJH04502']\n",
    "stim_trial_counter = 0\n",
    "if pID not in special_cases:\n",
    "    for i in range(trial_info.shape[0]):\n",
    "        if trial_info.loc[i, 'Condition'] == 1:\n",
    "            trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter]\n",
    "            stim_trial_counter += 1\n",
    "        else:\n",
    "            trial_info.loc[i, 'Onset_Sync'] = np.nan\n",
    "else:\n",
    "    if pID == 'BJH02503':\n",
    "        # Sync pulses not present for 1st few trials, but LFP shows stim\n",
    "        for i in range(trial_info.shape[0]):\n",
    "            if trial_info.loc[i, 'Condition'] == 1:\n",
    "                if stim_trial_counter < 6:\n",
    "                    trial_info.loc[i, 'Onset_Sync'] = trial_info.loc[i, 'Onset_SC'] - 18000 # empirical correction\n",
    "                else:\n",
    "                    trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter-6]\n",
    "                stim_trial_counter += 1\n",
    "            else:\n",
    "                trial_info.loc[i, 'Onset_Sync'] = np.nan\n",
    "                \n",
    "    if pID == 'BJH02703':\n",
    "        # Many sync pulses not present\n",
    "        fixed = False\n",
    "        for i in range(trial_info.shape[0]):\n",
    "            if trial_info.loc[i, 'Condition'] == 1:\n",
    "                if (stim_trial_counter >= 27) and (stim_trial_counter < 40):\n",
    "                    trial_info.loc[i, 'Onset_Sync'] = trial_info.loc[i, 'Onset_SC'] - 10000 # empirical correction\n",
    "                    fixed = True\n",
    "                else:\n",
    "                    if not fixed:\n",
    "                        trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter+1]\n",
    "                    else:\n",
    "                        trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter-13]\n",
    "                stim_trial_counter += 1\n",
    "                \n",
    "    elif pID == 'BJH04502':\n",
    "        # Missed detection of one pulse\n",
    "        fixed = False\n",
    "        for i in range(trial_info.shape[0]):\n",
    "            if trial_info.loc[i, 'Condition'] == 1:\n",
    "                if stim_trial_counter == 33:\n",
    "                    trial_info.loc[i, 'Onset_Sync'] = trial_info.loc[i, 'Onset_SC'] - 13000 # empirical correction\n",
    "                    fixed = True\n",
    "                else:\n",
    "                    if not fixed:\n",
    "                        trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter]\n",
    "                    else:\n",
    "                        trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter-1]\n",
    "                stim_trial_counter += 1\n",
    "            else:\n",
    "                trial_info.loc[i, 'Onset_Sync'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Define Stim/No-Stim Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Align Blackrock to BCI2000 w/ PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utah data does not have 1:1 mapping to timing in BCI2000, thus this step includes an empirical re-alignment to a common feature\n",
    "# present in both datasets, the photodiode (PD) signal. \n",
    "\n",
    "# Get PD onsets from trial_info\n",
    "PD_onsets = trial_info['Onset_PD'].values\n",
    "\n",
    "# Find peaks in PD channel (30 kHz); some trial onsets are +PD vs. -PD spike (depending on experiment)\n",
    "if pID in ['UIC20230201']:\n",
    "    peaks_30k, _ = find_peaks(PD*-1, height = 500, distance = 150000)\n",
    "else:\n",
    "    if fType == 'UIC':\n",
    "        peaks_30k, _ = find_peaks(PD, height = 500, distance = 150000)\n",
    "\n",
    "# Manual tuning (empirical correction from visual inspection)\n",
    "if pID == 'UIC20221301':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(2,47), np.arange(48, 93), np.arange(94, 139), np.arange(140, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20221501':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(2,47), np.arange(48, 93), np.arange(94, 139), np.arange(140, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20221701':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(3,48), np.arange(49, 94), np.arange(98, 143), np.arange(144, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "\n",
    "elif pID == 'UIC20230201':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(2,82), np.arange(83, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20230601':\n",
    "    matched_peaks = np.empty(len(trial_info)) * np.nan # initialize as nans; (missing PD for 17 trials)\n",
    "    keep_peaks = np.concatenate((np.arange(3,67), np.arange(68, 148), np.arange(150, 230), np.arange(231, len(peaks_30k)))) # realign\n",
    "    matched_peaks[16:] = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20230701':\n",
    "    peaks_30k = peaks_30k[3:] # remove spurious PDs\n",
    "    keep_peaks = np.concatenate((np.arange(0,12), np.arange(13,81), np.arange(82,162), np.arange(163,243), np.arange(244,324))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20230801':\n",
    "    peaks_30k = peaks_30k[4:] # remove spurious PDs\n",
    "    keep_peaks = np.concatenate((np.arange(0,80), np.arange(81,161), np.arange(162,242), np.arange(243,323))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20231101':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(3,83), np.arange(84, 164), np.arange(165, 245), np.arange(247, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20231401':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(2,122), np.arange(123, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20240101':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(3,123), np.arange(124, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "# O/S cohort (must remove scrambled images)\n",
    "if pID in ['UIC20221301', 'UIC20221501', 'UIC20221701']:\n",
    "    PD_block_idxs = [np.arange(0, 40), np.arange(40, 80), np.arange(80, 120), np.arange(120, 160)]\n",
    "    peaks_block_idxs = [np.arange(0, 45), np.arange(45, 90), np.arange(90, 135), np.arange(135, 180)]\n",
    "    \n",
    "    # Get peaks and onsets for each block\n",
    "    matched_peaks_noscramb = []\n",
    "    for i in range(len(PD_block_idxs)):\n",
    "        peaks_block = matched_peaks[peaks_block_idxs[i]]\n",
    "        PD_onsets_block = PD_onsets[PD_block_idxs[i]]\n",
    "\n",
    "        # Adjust PD onsets to align with 1st peak in block (\"blocks\" are separate .dat files stitched together)\n",
    "        PD_onsets_adj = PD_onsets_block + (peaks_block[0] - PD_onsets_block[0])\n",
    "\n",
    "        # Get the values in peaks closest to PD_onsets_adj\n",
    "        peaks_adj_block = []\n",
    "        for i in range(len(PD_onsets_adj)):\n",
    "            peaks_adj_block.append(matched_peaks[np.argmin(np.abs(matched_peaks - PD_onsets_adj[i]))])\n",
    "        matched_peaks_noscramb.append(peaks_adj_block)\n",
    "        \n",
    "    matched_peaks = np.array([item for sublist in matched_peaks_noscramb for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize\n",
    "# print('Detected Peaks: %i' %len(peaks_30k))\n",
    "# print('Matched Peaks: %i/%i' %(len(matched_peaks), len(trial_info)))\n",
    "\n",
    "# fig, ax = plt.subplots(figsize = (90, 3))\n",
    "# plt.plot(PD)\n",
    "# for x in peaks_30k:\n",
    "#     plt.axvline(x = x, color = 'r', linestyle = '-')\n",
    "# plt.plot(matched_peaks, PD[matched_peaks], 'x')\n",
    "# plt.plot(sync_onsets, PD[sync_onsets], 'o')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite Utah trial_info PD onsets using matched_peaks\n",
    "if fType == 'UIC':\n",
    "    trial_info['Onset_PD'] = matched_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.2 Use Sync & PD to Define Stim/No-Stim Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_epochs = []\n",
    "nostim_epochs = []\n",
    "\n",
    "for i in range(trial_info.shape[0]):\n",
    "    # stim trial\n",
    "    if trial_info.loc[i, 'Condition'] == 1:\n",
    "        stim_epochs.append(np.arange(trial_info.loc[i, 'Onset_Sync'] - (4 * fs), trial_info.loc[i, 'Onset_Sync'] + (2*fs)))\n",
    "    # no stim trial\n",
    "    else:\n",
    "        if np.isnan(trial_info['Onset_PD'][i]):\n",
    "            nostim_epochs.append(np.zeros(fs*6)) # populate empty epochs with zeros (cannot use np.nan in pd.DataFrame)\n",
    "        else:\n",
    "            nostim_epochs.append(np.arange(trial_info.loc[i, 'Onset_PD'] - (4 * fs), trial_info.loc[i, 'Onset_PD'] + (2*fs)))\n",
    "        \n",
    "stim_epochs = pd.DataFrame(stim_epochs).astype(int).T\n",
    "nostim_epochs = pd.DataFrame(nostim_epochs).astype(int).T\n",
    "\n",
    "stim_epochs.to_csv(os.path.join(save_path, 'StimEpochs.csv'))\n",
    "nostim_epochs.to_csv(os.path.join(save_path, 'NoStimEpochs.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Epoch 30 kHz Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_epochs_30k = []\n",
    "nostim_epochs_30k = []\n",
    "\n",
    "for i in range(stim_epochs.shape[1]):\n",
    "    stim_epochs_30k.append(cont_data[:, stim_epochs[i].values])\n",
    "    \n",
    "for i in range(nostim_epochs.shape[1]):\n",
    "    nostim_epochs_30k.append(cont_data[:, nostim_epochs[i].values])\n",
    "    \n",
    "stim_epochs_30k = np.array(stim_epochs_30k)\n",
    "nostim_epochs_30k = np.array(nostim_epochs_30k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Extract Peri-Stimulation Spike Counts\n",
    "Calculate the # spikes in the pre-/during-/post-stim windows (1s each). Export results as `PSCounts.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPSCounts(epochs, unit):\n",
    "    '''\n",
    "    Get peri-stimulation counts for a given unit (pre/during/post), exports a pd.DataFrame.\n",
    "    \n",
    "    Arguments:\n",
    "        epochs (np.ndarray): array of peri-stimulation epochs (indices)\n",
    "        unit (str): unit label\n",
    "        \n",
    "    Returns:\n",
    "        PSCountsDF (pd.DataFrame): peri-stimulation counts\n",
    "    '''\n",
    "    \n",
    "    # Get unit data\n",
    "    unitDF = events.copy()\n",
    "    unitDF['Chan-Unit'] = unitDF['Channel'].astype(str) + '-' + unitDF['Unit'].astype(str)\n",
    "    unitDF = unitDF[unitDF['Chan-Unit'] == unit]\n",
    "    unitDF['TimeStamps'] = unitDF['TimeStamps'].astype('int')\n",
    "\n",
    "    # Store counts for pre/during/post stim\n",
    "    preISICounts = 0\n",
    "    preCounts = 0\n",
    "    duringCounts = 0\n",
    "    postCounts = 0\n",
    "\n",
    "    # Plot raster\n",
    "    for i in range(epochs.shape[1]):\n",
    "        epoch_start = epochs[i].min()\n",
    "        stim_start = epoch_start + (4*fs)\n",
    "        epoch_end = epochs[i].max()\n",
    "        epochDF = unitDF[(unitDF['TimeStamps'] >= epoch_start) & (unitDF['TimeStamps'] <= epoch_end)]\n",
    "        epochDF = epochDF.reset_index(drop = True)\n",
    "        for spike in range(epochDF.shape[0]):\n",
    "            spike_time = (epochDF.iloc[spike]['TimeStamps'] - stim_start) / fs # set stim at t0, convert samples -> sec\n",
    "            if spike_time < -3:\n",
    "                preISICounts += 1\n",
    "            elif spike_time > -1 and spike_time < 0:\n",
    "                preCounts += 1\n",
    "            elif spike_time > 0 and spike_time < 1:\n",
    "                duringCounts += 1\n",
    "            elif spike_time > 1:\n",
    "                postCounts += 1\n",
    "                \n",
    "    # Construct DF of counts\n",
    "    PSCountsDF = pd.DataFrame({'PreISI': [preISICounts], 'Pre': [preCounts], 'During': [duringCounts], 'Post': [postCounts]})\n",
    "    PSCountsDF['pID'] = pID\n",
    "    PSCountsDF['Channel'] = unit.split('-')[0]\n",
    "    PSCountsDF['Unit'] = unit.split('-')[1]\n",
    "    \n",
    "    return PSCountsDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get peri-stimulation counts for each unit\n",
    "stim_counts = []\n",
    "nostim_counts = []\n",
    "\n",
    "for i in range(len(unit_labels)):\n",
    "    stim_counts.append(getPSCounts(epochs = stim_epochs, unit = unit_labels[i]))\n",
    "stim_counts = pd.concat(stim_counts)\n",
    "stim_counts['Condition'] = 'Stim'\n",
    "\n",
    "for i in range(len(unit_labels)):\n",
    "    nostim_counts.append(getPSCounts(epochs = nostim_epochs, unit = unit_labels[i]))\n",
    "nostim_counts = pd.concat(nostim_counts)\n",
    "nostim_counts['Condition'] = 'No-Stim'\n",
    "\n",
    "PSCountsDF = pd.concat([stim_counts, nostim_counts])\n",
    "PSCountsDF.reset_index(inplace = True, drop = True)\n",
    "PSCountsDF.to_csv((os.path.join(save_path, 'PSCounts.csv')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview color palette\n",
    "sns.color_palette('flare', summaryDF.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Unit Waveforms\n",
    "Use `plotUnitWaveforms()` and `plotUnitWaveforms_RAW()` to generate separate figures for each channel showing the waveforms of the units detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotUnitWaveforms(chan, show = True, save = False):\n",
    "    '''\n",
    "    Plot unit waveforms.\n",
    "    \n",
    "    Arguments:\n",
    "        chan (str): channel label\n",
    "        show (bool): show figure\n",
    "        save (bool): save figure\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    # Parset events for chan data\n",
    "    unitDF = events[events['Channel'] == chan]\n",
    "    n_units = len(unitDF['Unit'].unique())\n",
    "    unit_idxs = unitDF.index.values\n",
    "    \n",
    "    # Parse waveforms using unit indices\n",
    "    unit_waveforms = pd.DataFrame(waveforms[unit_idxs])\n",
    "    \n",
    "    # Add unit labels\n",
    "    unit_waveforms['Unit'] = unitDF['Unit'].reset_index(drop=True)\n",
    "    \n",
    "    # Track waveform counts for legend\n",
    "    waveform_counts = unitDF['Unit'].value_counts().values.tolist()\n",
    "    waveform_counts = [str(x) for x in waveform_counts]\n",
    "    \n",
    "    # Melt DataFrame for plotting in Seaborn\n",
    "    unit_waveforms = unit_waveforms.melt(id_vars = ['Unit'], var_name = 'Time', value_name = 'Voltage')\n",
    "    \n",
    "    # Figure parameters\n",
    "    fig, ax = plt.subplots(1, 1, figsize = (5,2.5))\n",
    "    palette = ['#ff6e61', '#ffb84d', '#6d9dc5', '#5e4b8b']\n",
    "    \n",
    "    # Adjust time (samples -> ms)\n",
    "    unit_waveforms['Time'] = (unit_waveforms['Time'] / fs) * 1000\n",
    "    \n",
    "    # Plot average waveform ± SD (shaded)\n",
    "    sns.lineplot(x = 'Time', y = 'Voltage', hue = 'Unit', data = unit_waveforms, palette = palette, linewidth = 2, errorbar = 'sd', ax = ax)\n",
    "    \n",
    "    # Figure aeshetics\n",
    "    if fType == 'UIC':\n",
    "        plt.xlim([0, 1.5])\n",
    "        plt.xticks([0, 0.5, 1, 1.5])\n",
    "    elif fType == 'BJH':\n",
    "        plt.xlim([0, 1])\n",
    "        plt.xticks([0, 0.5, 1])\n",
    "    plt.xlabel('Time (ms)', fontsize = 'large')\n",
    "    plt.yticks([-100, -50, 0, 50, 100])\n",
    "    plt.ylabel('Voltage ($\\mu$V)', fontsize = 'large')\n",
    "    plt.title(chan, fontsize = 'x-large', fontweight = 'bold')\n",
    "    legend_handles, _= ax.get_legend_handles_labels()\n",
    "    ax.legend(legend_handles, waveform_counts, title = 'WFs', loc = 'lower right', fontsize = 'x-small', title_fontsize = 'x-small')\n",
    "    sns.despine(top = True, right = True)\n",
    "    \n",
    "    # Export figure\n",
    "    if save:\n",
    "        if not os.path.exists(os.path.join(save_path, 'Units')):\n",
    "            os.mkdir(os.path.join(save_path, 'Units'))\n",
    "        plt.savefig(os.path.join(save_path, 'Units', chan + '.pdf'), dpi = 1500, bbox_inches = 'tight')\n",
    "    \n",
    "    if show == False:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotUnitWaveforms_RAW(chan, show = True, save = False):\n",
    "    '''\n",
    "    Plot unit waveforms and visualize raw traces.\n",
    "    \n",
    "    Arguments:\n",
    "        chan (str): channel label\n",
    "        show (bool): show figure\n",
    "        save (bool): save figure\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    # Parset events for chan data\n",
    "    unitDF = events[events['Channel'] == chan]\n",
    "    n_units = len(unitDF['Unit'].unique())\n",
    "    palette = ['#ff6e61', '#ffb84d', '#6d9dc5', '#5e4b8b']\n",
    "    \n",
    "    for i in unitDF['Unit'].unique():\n",
    "        unitDFsub = unitDF[unitDF['Unit'] == i]\n",
    "        unit_idxs = unitDFsub.index.values\n",
    "        \n",
    "        # Parse waveforms using unit indices\n",
    "        unit_waveforms = pd.DataFrame(waveforms[unit_idxs])\n",
    "        \n",
    "        # Figure parameters\n",
    "        fig, ax = plt.subplots(1, 1, figsize = (5,2.5))\n",
    "\n",
    "        # Add unit and spike labels\n",
    "        unit_waveforms['Unit'] = unitDFsub['Unit'].reset_index(drop=True)\n",
    "        unit_waveforms['Spike'] = unit_waveforms.index\n",
    "        \n",
    "        # Track waveform counts for legend\n",
    "        waveform_counts = unitDFsub['Unit'].value_counts().values[0]\n",
    "        # waveform_counts = [str(x) for x in waveform_counts]\n",
    "        \n",
    "        # Melt DataFrame for plotting in Seaborn\n",
    "        unit_waveforms = unit_waveforms.melt(id_vars = ['Unit', 'Spike'], var_name = 'Time', value_name = 'Voltage')\n",
    "        \n",
    "        # Adjust time (samples -> ms)\n",
    "        unit_waveforms['Time'] = (unit_waveforms['Time'] / fs) * 1000\n",
    "        \n",
    "        for wf in range(unit_waveforms['Spike'].max()):\n",
    "            plt.plot(unit_waveforms[unit_waveforms['Spike'] == wf]['Time'], unit_waveforms[unit_waveforms['Spike'] == wf]['Voltage'], lw = 0.1, color = '#D3D3D3', alpha = 0.5, axes = ax)\n",
    "            \n",
    "        # Plot average waveform\n",
    "        sns.lineplot(x = 'Time', y = 'Voltage', data = unit_waveforms, color = palette[i-1], linewidth = 3, errorbar = None, ax = ax, label = waveform_counts)    \n",
    "        \n",
    "        # Figure aeshetics\n",
    "        if fType == 'UIC':\n",
    "            plt.xlim([0, 1.5])\n",
    "            plt.xticks([0, 0.5, 1, 1.5])\n",
    "        elif fType == 'BJH':\n",
    "            plt.xlim([0, 1])\n",
    "            plt.xticks([0, 0.5, 1])\n",
    "        plt.xlabel('Time (ms)', fontsize = 'large')\n",
    "        plt.yticks([-100, -50, 0, 50, 100])\n",
    "        plt.ylabel('Voltage ($\\mu$V)', fontsize = 'large')\n",
    "        plt.title((chan + '-' + str(i)), fontsize = 'x-large', fontweight = 'bold')\n",
    "        legend_handles, legend_labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(legend_handles, legend_labels, title = 'WFs', loc = 'lower right', fontsize = 'x-small', title_fontsize = 'x-small')\n",
    "        sns.despine(top = True, right = True)\n",
    "        \n",
    "        # Export figure\n",
    "        if save:\n",
    "            if not os.path.exists(os.path.join(save_path, 'Units')):\n",
    "                os.mkdir(os.path.join(save_path, 'Units'))\n",
    "            plt.savefig(os.path.join(save_path, 'Units', 'RAW_' + chan + '-' + str(i) + '.pdf'), dpi = 1500, bbox_inches = 'tight')\n",
    "        \n",
    "        if show == False:\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot unit waveforms for each channel (& export as .pdfs)\n",
    "for chan in events['Channel'].unique():\n",
    "    plotUnitWaveforms(chan = chan, show = False, save = True)\n",
    "    plotUnitWaveforms_RAW(chan = chan, show = False, save = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Unit Spike Rasters\n",
    "Use `plotFullRaster()` to generate a raster plot showing unit activity across the entire recording."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotFullRaster(show = True, save = False):\n",
    "    '''\n",
    "    Plot raster of all units for duration of recording.\n",
    "    \n",
    "    Arguments:\n",
    "        show (bool): show figure\n",
    "        save (bool): save figure\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # Figure parameters\n",
    "    if summaryDF.shape[0] > 5:\n",
    "        fig = plt.figure(figsize = (10, summaryDF.shape[0] * 0.35), constrained_layout=True)\n",
    "    else:\n",
    "        fig = plt.figure(figsize = (10, 3), constrained_layout=True)\n",
    "    gs = gridspec.GridSpec(ncols=10, nrows=1, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[0:9])\n",
    "    ax2 = fig.add_subplot(gs[9])\n",
    "    palette = sns.color_palette('flare_r', n_colors = summaryDF.shape[0])\n",
    "\n",
    "    # Sort events by Channel\n",
    "    rasterDF = events.copy().sort_values(by = ['Channel', 'Unit'], ascending = False)\n",
    "\n",
    "    # Create iterator and list to track unit label and position\n",
    "    i = 0\n",
    "    unit_labels = []\n",
    "\n",
    "    # Loop through each channel, each unit, and each spike to plot spike times\n",
    "    for chan in rasterDF['Channel'].unique():\n",
    "        chanDF = rasterDF[rasterDF['Channel'] == chan]\n",
    "        for unit in chanDF['Unit'].unique():\n",
    "            unitDF = chanDF[chanDF['Unit'] == unit]\n",
    "            unit_labels.append(unitDF['Channel'].unique()[0] + '-' + str(unitDF['Unit'].unique()[0]))\n",
    "            i += 1\n",
    "            for spike in range(unitDF.shape[0]):\n",
    "                spike_time = unitDF.iloc[spike]['TimeStamps']\n",
    "                spike_time = spike_time / fs / 60 # convert from samples -> min\n",
    "                ax1.vlines(spike_time, i - 0.4, i + 0.4, linewidth = 0.25, colors = palette[i-1])\n",
    "                \n",
    "    # Add barplot for number of waveforms\n",
    "    waveCountDF = summaryDF.copy().reset_index()\n",
    "    waveCountDF['Chan-Unit'] = waveCountDF['Channel'].astype(str) + '-' + waveCountDF['Unit'].astype(str)\n",
    "    waveCountDF = waveCountDF.sort_values('Chan-Unit', ascending = False)\n",
    "    sns.barplot(x = 'Waveforms', y = 'Chan-Unit', data = waveCountDF, ax = ax2, palette = palette)\n",
    "\n",
    "    # Raster plot aesthetics\n",
    "    ax1.set_ylim([0, summaryDF.shape[0] + 1])\n",
    "    ax1.set_yticks(np.arange(1, summaryDF.shape[0] + 1), unit_labels)\n",
    "    ax1.set_ylabel('')\n",
    "    ax1.set_xlim([0, (len(sync) / fs / 60)])\n",
    "    ax1.set_xlabel('Time (min)', fontsize = 'large')\n",
    "    \n",
    "    # Bar plot aesthetics\n",
    "    ax2.set_ylim([-1, summaryDF.shape[0]])\n",
    "    ax2.set_yticks(np.arange(0, summaryDF.shape[0]), unit_labels)\n",
    "    ax2.set_ylabel('')\n",
    "    ax2.set_yticklabels([])\n",
    "    ax2.set_xlim([0, np.ceil(summaryDF['Waveforms'].max()/100) * 100])\n",
    "    ax2.set_xticks([0, np.ceil(summaryDF['Waveforms'].max()/100) * 100])\n",
    "    ax2.set_xlabel('WFs', fontsize = 'large')\n",
    "    \n",
    "    # Figure aesthetics\n",
    "    plt.suptitle(pID, fontweight = 'bold', fontsize = 'x-large')\n",
    "    sns.despine(top = True, right = True)\n",
    "\n",
    "    # Export figure\n",
    "    if save:\n",
    "        if not os.path.exists(os.path.join(save_path, 'Rasters')):\n",
    "            os.mkdir(os.path.join(save_path, 'Rasters'))\n",
    "        plt.savefig(os.path.join(save_path, 'Rasters', 'FullRaster.pdf'), dpi = 1500, bbox_inches = 'tight')\n",
    "\n",
    "    if show == False:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot raster plot of all units during full recording (& export as .pdf)\n",
    "plotFullRaster(show=False, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Peri-Stim Spike Rasters\n",
    "Use `plotPSRaster()` and `plotPSRaster_Extended()`to generate a peri-stim raster plot for a unique unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPSRaster(epochs, unit, color_idx, show = True, save = False):\n",
    "    '''\n",
    "    Plot unit raster for peri-stimulation epochs.\n",
    "    \n",
    "    Arguments:\n",
    "        epochs (np.ndarray): array of peri-stimulation epochs (indices)\n",
    "        unit (str): unit label\n",
    "        color_idx (int): index of color in palette\n",
    "        show (bool): show figure\n",
    "        save (bool): save figure\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # Figure parameters\n",
    "    # fig, axes = plt.subplots(2, 1, figsize = (4, 4))\n",
    "    fig = plt.figure(figsize = (4,4), constrained_layout = True)\n",
    "    gs = gridspec.GridSpec(ncols=1, nrows=8, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    ax2 = fig.add_subplot(gs[1:6])\n",
    "    ax3 = fig.add_subplot(gs[6:8])\n",
    "    \n",
    "    palette = ['#ff6e61', '#ffb84d', '#6d9dc5', '#5e4b8b']\n",
    "\n",
    "    # Get unit data\n",
    "    unitDF = events.copy()\n",
    "    unitDF['Chan-Unit'] = unitDF['Channel'].astype(str) + '-' + unitDF['Unit'].astype(str)\n",
    "    unitDF = unitDF[unitDF['Chan-Unit'] == unit]\n",
    "    unitDF['TimeStamps'] = unitDF['TimeStamps'].astype('int')\n",
    "    n_trials = epochs.shape[1]\n",
    "    \n",
    "    # Get LFP from 30 kHz data, highpass filter, and plot\n",
    "    nyquist = 0.5 * fs\n",
    "    hpass_freq = 150\n",
    "    hpass_cutoff = hpass_freq / nyquist\n",
    "    b, a = butter(4, hpass_cutoff, btype = 'high')\n",
    "    chan_idx = chan_labels.index(unit.split('-')[0])\n",
    "    lfp_time = np.arange(-1, 2, 1/fs)\n",
    "    lfp_data = filtfilt(b, a, stim_epochs_30k[:,chan_idx,(fs*3):], axis = 1)\n",
    "    ax1.plot(lfp_time, np.mean(lfp_data, axis = 0), color = palette[color_idx], lw = 1)\n",
    "\n",
    "    # Plot raster\n",
    "    epochSpikes = []\n",
    "    for i in range(n_trials):\n",
    "        epoch_start = epochs[i].min()\n",
    "        stim_start = epoch_start + (4*fs)\n",
    "        epoch_end = epochs[i].max()\n",
    "        epochDF = unitDF[(unitDF['TimeStamps'] >= epoch_start) & (unitDF['TimeStamps'] <= epoch_end)]\n",
    "        epochDF = epochDF.reset_index(drop = True)\n",
    "        epochDF['TimeAdj'] = (epochDF['TimeStamps'] - stim_start) / fs\n",
    "        epochSpikes.append(epochDF)\n",
    "        for spike in range(epochDF.shape[0]):\n",
    "            spike_time = (epochDF.iloc[spike]['TimeStamps'] - stim_start) / fs # set stim at t0, convert samples -> sec\n",
    "            y_pos = i + 1\n",
    "            ax2.vlines(spike_time, y_pos - 0.4, y_pos + 0.4, linewidth = 2, color = palette[color_idx])\n",
    "            \n",
    "    # Plot binned FR\n",
    "    spikesDF = pd.concat(epochSpikes)\n",
    "    spikesDF = spikesDF.reset_index(drop = True)\n",
    "    binSize = 0.1 # time (s)\n",
    "    bins = np.arange(-1, 2.1, binSize)\n",
    "    maxSpikes = np.histogram(spikesDF['TimeAdj'], bins = bins)[0].max()\n",
    "    ps_spike_win = spikesDF.copy()\n",
    "    ps_spike_win = ps_spike_win[ps_spike_win['TimeAdj'] >= -1]\n",
    "    ps_spike_win = ps_spike_win[ps_spike_win['TimeAdj'] <= 2]\n",
    "    sns.histplot(ps_spike_win['TimeAdj'], bins = bins, kde = True, kde_kws = {'bw_adjust': 0.2, 'cut': 3, 'clip': [-1,2]}, color = palette[color_idx], ax = ax3)\n",
    "\n",
    "    # Shade times where BLA was stimulated\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.axvspan(0, 1, color = 'grey', alpha = 0.1, zorder = -10)\n",
    "        ax.axvline(0, color = 'k', linestyle = '--', lw = 0.5)\n",
    "        ax.axvline(1, color = 'k', linestyle = '--', lw = 0.5)\n",
    "        ax.set_xlim([-1, 2.05])\n",
    "        sns.despine(top = True, right = True)\n",
    "\n",
    "    # Figure aesthetics\n",
    "    ax1.set_xticks([-1, 0, 1, 2], ['', '', '', ''])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlim([-1, 2.01])\n",
    "    ax2.set_xlabel('')\n",
    "    ax2.set_xticks([-1, 0, 1, 2], ['', '', '', ''])\n",
    "    ax2.set_ylabel('Trial', fontsize = 'x-large', labelpad= 10)\n",
    "    ax2.set_ylim([0, n_trials+1])\n",
    "    ax2.set_xlim([-1, 2.01])\n",
    "    ax2.set_yticks([1, n_trials], ['1', str(n_trials)], fontsize = 'medium')\n",
    "    ax2.set_xlabel('')\n",
    "    ax3.set_xlabel('Time (s)', fontsize = 'x-large')\n",
    "    ax3.set_xticks([-1, 0, 1, 2], ['-1', '0', '1', '2'], fontsize = 'medium')\n",
    "    ax3.set_ylabel('FR (Hz)', fontsize = 'x-large', labelpad = 7)\n",
    "    ax3.set_yticks([0, maxSpikes], [0, np.round((maxSpikes / n_trials) * 10, 1)], fontsize = 'medium')\n",
    "    ax3.set_xlim([-1, 2.01])\n",
    "    ax1.set_title(unit, fontsize = 'x-large', fontweight = 'bold', pad = 15)\n",
    "    sns.despine(top = True, right = True, left = True, ax = ax1)\n",
    "    \n",
    "\n",
    "    # # Export figure\n",
    "    if save:\n",
    "        if not os.path.exists(os.path.join(save_path, 'Rasters')):\n",
    "            os.mkdir(os.path.join(save_path, 'Rasters'))\n",
    "        plt.savefig(os.path.join(save_path, 'Rasters', unit + '_PSRaster.pdf'), dpi = 1500, bbox_inches = 'tight')\n",
    "\n",
    "    if show == False:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPSRaster_Extended(epochs, unit, color_idx, show = True, save = False):\n",
    "    '''\n",
    "    Plot unit raster for extended (full) peri-stimulation epochs.\n",
    "    \n",
    "    Arguments:\n",
    "        epochs (np.ndarray): array of peri-stimulation epochs (indices)\n",
    "        unit (str): unit label\n",
    "        color_idx (int): index of color in palette\n",
    "        show (bool): show figure\n",
    "        save (bool): save figure\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    # Figure parameters\n",
    "    # fig, axes = plt.subplots(2, 1, figsize = (4, 4))\n",
    "    fig = plt.figure(figsize = (8,4), constrained_layout = True)\n",
    "    gs = gridspec.GridSpec(ncols=1, nrows=8, figure=fig)\n",
    "    ax1 = fig.add_subplot(gs[0])\n",
    "    ax2 = fig.add_subplot(gs[1:6])\n",
    "    ax3 = fig.add_subplot(gs[6:8])\n",
    "    \n",
    "    palette = ['#ff6e61', '#ffb84d', '#6d9dc5', '#5e4b8b']\n",
    "\n",
    "    # Get unit data\n",
    "    unitDF = events.copy()\n",
    "    unitDF['Chan-Unit'] = unitDF['Channel'].astype(str) + '-' + unitDF['Unit'].astype(str)\n",
    "    unitDF = unitDF[unitDF['Chan-Unit'] == unit]\n",
    "    unitDF['TimeStamps'] = unitDF['TimeStamps'].astype('int')\n",
    "    n_trials = epochs.shape[1]\n",
    "    \n",
    "    # Get LFP from 30 kHz data, highpass filter, and plot\n",
    "    nyquist = 0.5 * fs\n",
    "    hpass_freq = 150\n",
    "    hpass_cutoff = hpass_freq / nyquist\n",
    "    b, a = butter(4, hpass_cutoff, btype = 'high')\n",
    "    chan_idx = chan_labels.index(unit.split('-')[0])\n",
    "    lfp_time = np.arange(-4, 2, 1/fs)\n",
    "    lfp_data = filtfilt(b, a, stim_epochs_30k[:,chan_idx,:], axis = 1)\n",
    "    ax1.plot(lfp_time, np.mean(lfp_data, axis = 0), color = palette[color_idx], lw = 1)\n",
    "\n",
    "    # Plot raster\n",
    "    epochSpikes = []\n",
    "    for i in range(n_trials):\n",
    "        epoch_start = epochs[i].min()\n",
    "        stim_start = epoch_start + (4*fs)\n",
    "        epoch_end = epochs[i].max()\n",
    "        epochDF = unitDF[(unitDF['TimeStamps'] >= epoch_start) & (unitDF['TimeStamps'] <= epoch_end)]\n",
    "        epochDF = epochDF.reset_index(drop = True)\n",
    "        epochDF['TimeAdj'] = (epochDF['TimeStamps'] - stim_start) / fs\n",
    "        epochSpikes.append(epochDF)\n",
    "        for spike in range(epochDF.shape[0]):\n",
    "            spike_time = (epochDF.iloc[spike]['TimeStamps'] - stim_start) / fs # set stim at t0, convert samples -> sec\n",
    "            y_pos = i + 1\n",
    "            ax2.vlines(spike_time, y_pos - 0.4, y_pos + 0.4, linewidth = 2, color = palette[color_idx])\n",
    "            \n",
    "    # Plot binned FR\n",
    "    spikesDF = pd.concat(epochSpikes)\n",
    "    spikesDF = spikesDF.reset_index(drop = True)\n",
    "    binSize = 0.1 # time (s)\n",
    "    bins = np.arange(-4, 2.1, binSize)\n",
    "    maxSpikes = np.histogram(spikesDF['TimeAdj'], bins = bins)[0].max()\n",
    "    hist = sns.histplot(spikesDF['TimeAdj'], bins = bins, kde = True, kde_kws = {'bw_adjust': 0.2, 'cut': 3, 'clip': [-4,2]}, color = palette[color_idx], ax = ax3)\n",
    "    \n",
    "    # get kde values from histplot on ax3\n",
    "    KDE_X, KDE_Y = hist.get_lines()[0].get_data()\n",
    "    KDE_DF = pd.DataFrame({'X': KDE_X, 'Y': KDE_Y})\n",
    "\n",
    "    # Shade times where BLA was stimulated\n",
    "    for ax in [ax1, ax2, ax3]:\n",
    "        ax.axvspan(0, 1, color = 'grey', alpha = 0.1, zorder = -10)\n",
    "        ax.axvline(-3, color = 'k', linestyle = '--', lw = 0.5)\n",
    "        ax.axvline(0, color = 'k', linestyle = '--', lw = 0.5)\n",
    "        ax.axvline(1, color = 'k', linestyle = '--', lw = 0.5)\n",
    "        ax.set_xlim([-4, 2.05])\n",
    "        sns.despine(top = True, right = True)\n",
    "    \n",
    "    # Add text above ax1\n",
    "    ax1.text(0.055, 1.35, 'Pre-ISI', fontsize = 'medium', transform = ax1.transAxes)\n",
    "    ax1.text(0.375, 1.35, 'Image', fontsize = 'medium', transform = ax1.transAxes)\n",
    "    ax1.text(0.675, 1.35, 'Stim | No-Stim', fontsize = 'medium', transform = ax1.transAxes)\n",
    "    ax1.text(0.875, 1.35, 'Post-ISI', fontsize = 'medium', transform = ax1.transAxes)\n",
    "        \n",
    "    # Figure aesthetics\n",
    "    ax1.set_xticks([-4, -3, -2, -1, 0, 1, 2], ['', '', '', '', '', '', ''])\n",
    "    ax1.set_yticks([])\n",
    "    ax1.set_xlim([-4, 2.01])\n",
    "    ax2.set_xlabel('')\n",
    "    ax2.set_xticks([-4, -3, -2, -1, 0, 1, 2], ['', '', '', '', '', '', ''])\n",
    "    ax2.set_ylabel('Trial', fontsize = 'x-large', labelpad= 10)\n",
    "    ax2.set_ylim([0, n_trials+1])\n",
    "    ax2.set_xlim([-4, 2.01])\n",
    "    ax2.set_yticks([1, n_trials], ['1', str(n_trials)], fontsize = 'medium')\n",
    "    ax2.set_xlabel('')\n",
    "    ax3.set_xlabel('Time (s)', fontsize = 'x-large', labelpad = 20)\n",
    "    ax3.set_xticks([-4, -3, -2, -1, 0, 1, 2], ['-4', '-3', '-2', '-1', '0', '1', '2'], fontsize = 'medium')\n",
    "    ax3.set_ylabel('FR (Hz)', fontsize = 'x-large', labelpad = 7)\n",
    "    ax3.set_yticks([0, maxSpikes], [0, np.round((maxSpikes / n_trials) * 10, 1)], fontsize = 'medium')\n",
    "    ax3.set_xlim([-4, 2.01])\n",
    "    ax1.set_title(unit, fontsize = 'x-large', fontweight = 'bold', pad = 35)\n",
    "    sns.despine(top = True, right = True, left = True, ax = ax1)\n",
    "    \n",
    "\n",
    "    # Export figure\n",
    "    if not os.path.exists(os.path.join(save_path, 'Rasters')):\n",
    "        os.mkdir(os.path.join(save_path, 'Rasters'))\n",
    "    plt.savefig(os.path.join(save_path, 'Rasters', unit + '_ExtendedPSRaster.pdf'), dpi = 1500, bbox_inches = 'tight')\n",
    "\n",
    "    if show == False:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot peri-stim rasters for each unit (& export as .pdf)\n",
    "for i in range(len(unit_labels)):\n",
    "    plotPSRaster(epochs = stim_epochs, unit = unit_labels[i], color_idx = i, show = False, save = True)\n",
    "    plotPSRaster_Extended(epochs = stim_epochs, unit = unit_labels[i], color_idx = i, show = False, save = True)\n",
    "\n",
    "# For generating example rasters in Fig2\n",
    "# plotPSRaster(epochs = stim_epochs, unit = 'mROFC7-1', color_idx = 1, show = True, save = True) # P6\n",
    "# plotPSRaster(epochs = stim_epochs, unit = 'mROFC8-1', color_idx = 1, show = True, save = True) # P6\n",
    "# plotPSRaster(epochs = stim_epochs, unit = 'mRHIP3-1', color_idx = 0, show = True, save = True) # P9\n",
    "# plotPSRaster(epochs = stim_epochs, unit = 'mLAMY8-1', color_idx = 2, show = True, save = True) # P25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Epoch Validation\n",
    "Use `plotEpochValidation()` to plot the sync channel for each epoch to verify alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotEpochValidation(epochs, show = True, save = True):\n",
    "    '''\n",
    "    Plot sync data across epochs to validate alignment.\n",
    "    \n",
    "    Arguments:\n",
    "        show (bool): show figure\n",
    "        save (bool): save figure\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    n_rows = int(np.ceil(epochs.shape[1]/8))\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, 8, figsize = (n_rows+2, n_rows+2), constrained_layout = True)\n",
    "    for i in range(epochs.shape[1]):\n",
    "        ax = axes.flatten()[i]\n",
    "        ax.plot(sync[epochs[i]], color = 'k', linewidth = 0.5)\n",
    "        ax.axvline(fs*4, color = 'r', linewidth = 1)\n",
    "        ax.axvline(fs*5, color = 'r', linewidth = 1)\n",
    "        ax.axvspan(fs*4, fs*5, color = 'r', alpha = 0.1)\n",
    "        ax.set_xticks([fs*3, fs*4, fs*5, fs*6], ['-1', '0', '1', '2'], fontsize = 'x-small')\n",
    "        ax.set_xlim([fs*3, fs*6])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title('Epoch: %s' %str(i+1), fontsize = 'x-small')\n",
    "        sns.despine(right = True, top = True)\n",
    "\n",
    "    # if axis number is greater than number of epochs, remove axis\n",
    "    for i in range(epochs.shape[1], axes.size):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "        \n",
    "    # Export figure\n",
    "    if save:\n",
    "        if not os.path.exists(os.path.join(save_path, 'Validation')):\n",
    "            os.mkdir(os.path.join(save_path, 'Validation'))\n",
    "        plt.savefig(os.path.join(save_path, 'Validation', 'EpochsSync.pdf'), dpi = 1500, bbox_inches = 'tight')\n",
    "    \n",
    "    if show == False:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotEpochValidation(stim_epochs, show = False, save = True)\n",
    "# plotEpochValidation(nostim_epochs, show = False, save = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Processed %s' %pID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
