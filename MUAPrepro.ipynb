{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLAES Units MUA Preprocessing\n",
    "\n",
    "This notebook contains code that mirrors the main preprocessing code (`BLAESUnitPrepro.ipynb`), but calculates threshold crossings in the MUA signal rather than spiking of isolated neurons.\n",
    "\n",
    "---\n",
    "\n",
    "> *Author: Justin Campbell (justin.campbell@hsc.utah.edu)*  \n",
    "> *Version: 6/4/2025*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Notebook\n",
    "Import libraries, utilities, and packages. Configure notebook figure settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import mat73\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.io import loadmat\n",
    "from scipy.signal import find_peaks, filtfilt, find_peaks, firwin\n",
    "from scipy.fftpack import fft, ifft\n",
    "\n",
    "# Import Blackrock Python Utilities\n",
    "sys.path.append('/Users/justincampbell/Library/CloudStorage/GoogleDrive-u0815766@gcloud.utah.edu/My Drive/Research Projects/BLAESUnits/Code/Blacrock-Python-Utilities')\n",
    "import brpylib\n",
    "\n",
    "%matplotlib inline\n",
    "# %config InlineBackend.figure_format='retina'\n",
    "%config InlineBackend.figure_format='svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNEV(nev_path):\n",
    "    '''\n",
    "    Load .nev file and extract spike events and waveforms.\n",
    "    \n",
    "    Arguments:\n",
    "        nev_path (str): path to .nev file\n",
    "    Returns:\n",
    "        events (pd.DataFrame): spike events\n",
    "        waveforms (np.ndarray): spike waveforms\n",
    "    '''\n",
    "    \n",
    "    # Open .nev file\n",
    "    nev_file = brpylib.NevFile(nev_path)\n",
    "    \n",
    "    # Extract data\n",
    "    print('\\nLoading .nev data...')\n",
    "    nev_data = nev_file.getdata()['spike_events']\n",
    "    \n",
    "    # Close .nev file\n",
    "    nev_file.close()\n",
    "    \n",
    "    # Extract waveforms\n",
    "    waveforms = nev_data['Waveforms']\n",
    "    \n",
    "    # Convert spike events to pd.DataFrame\n",
    "    spikes = nev_data.copy()\n",
    "    spikes.pop('Waveforms', None)\n",
    "    events = pd.DataFrame.from_dict(spikes)\n",
    "    \n",
    "    # Define valid unit labels\n",
    "    valid_unit_labels = np.arange(1,17) # 0 = unclassified, 255 = noise, 1-16 = valid units\n",
    "    events = events[events['Unit'].isin(valid_unit_labels)]\n",
    "    valid_unit_idxs = events.index.values\n",
    "    waveforms = waveforms[valid_unit_idxs]\n",
    "    \n",
    "    # Reindex\n",
    "    events.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    return events, waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNSX(nsx_path):\n",
    "    '''\n",
    "    Load .nsx file and extract data and header.\n",
    "    \n",
    "    Arguments:\n",
    "        nsx_path (str): path to .nsx file\n",
    "        \n",
    "    Returns:\n",
    "        nsx_data (np.ndarray): raw neural data (30 kHz)\n",
    "        header (dict): header (recording info)\n",
    "    '''\n",
    "    # Open .nsx file\n",
    "    nsx_file = brpylib.NsxFile(nsx)\n",
    "\n",
    "    # Extract data & header\n",
    "    print('\\nLoading .nsx data...')\n",
    "    nsx_data = nsx_file.getdata(full_timestamps = True)['data'][0]\n",
    "    header = nsx_file.extended_headers\n",
    "\n",
    "    # Close .nev file\n",
    "    nsx_file.close()\n",
    "    \n",
    "    return nsx_data, header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadSortedMat(mat_path):\n",
    "    '''\n",
    "    Load sorted .mat file and extract data and chan_labels.\n",
    "    \n",
    "    Arguments:\n",
    "        mat_path (str): path to *sorted.mat file\n",
    "        \n",
    "    Returns:\n",
    "        mat_dict (dict): micro channels with sorted units\n",
    "        chan_labels (list): micro channel labels (all channels)\n",
    "    '''\n",
    "    # load the mat file\n",
    "    mat_dict = loadmat(mat_path, simplify_cells=True)\n",
    "\n",
    "    # load WashU unit labels\n",
    "    washU_labels = pd.read_csv('/Users/justincampbell/Library/CloudStorage/GoogleDrive-u0815766@gcloud.utah.edu/My Drive/Research Projects/BLAESUnits/WashU Micro Labels.csv')\n",
    "    lead_labels = washU_labels[washU_labels['pID'] == pID[:-2]]['Label'].values\n",
    "\n",
    "    # create a list of all channels\n",
    "    chan_labels = []\n",
    "    for i in range(len(lead_labels)):\n",
    "        for ii in range(1, 9):\n",
    "            chan_labels.append(lead_labels[i] + str(ii))\n",
    "\n",
    "    # remove variables with '__' in them\n",
    "    mat_dict = {k: v for k, v in mat_dict.items() if '__' not in k}\n",
    "\n",
    "    # remove Sync channel\n",
    "    if len(mat_dict.keys()) == 17:\n",
    "        mat_dict.pop('Channel17', None)\n",
    "    elif len(mat_dict.keys()) == 9:\n",
    "        mat_dict.pop('Channel09', None)\n",
    "\n",
    "    # rename keys with channel names\n",
    "    mat_dict = {chan_labels[i]: v for i, (k, v) in enumerate(mat_dict.items())}\n",
    "\n",
    "    # remove channels that are empty (no units)\n",
    "    mat_dict = {k: v for k, v in mat_dict.items() if v.size > 0}\n",
    "    \n",
    "    return mat_dict, chan_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_path = '/Volumes/Hippocampus/BLAESUnits/Data_30k'\n",
    "\n",
    "# Find relevant .nev, .nsx, and .mat files\n",
    "nev_files = glob.glob(root_dir = data_path, pathname = '**/*.nev', recursive = True)\n",
    "nsx_files = glob.glob(root_dir = data_path, pathname = '**/*.ns6', recursive = True)\n",
    "sorted_mat_files = glob.glob(root_dir = data_path, pathname = '**/*sorted.mat', recursive = True)\n",
    "raw_mat_files = glob.glob(root_dir = data_path, pathname = '**/*raw.mat', recursive = True)\n",
    "\n",
    "# Sort based on filename\n",
    "nev_files.sort()\n",
    "nsx_files.sort()\n",
    "sorted_mat_files.sort()\n",
    "raw_mat_files.sort()\n",
    "\n",
    "# Combine to create list of sessions\n",
    "sessions = [nev_files[i].split('/')[0] for i in range(len(nev_files))]\n",
    "for i in range(len(sorted_mat_files)):\n",
    "    sessions.append(sorted_mat_files[i].split('/')[0])\n",
    "\n",
    "# Data params\n",
    "fs = 30000 # sampling rate\n",
    "\n",
    "# Display\n",
    "print('Found data for %s sessions:' % (len(sessions)))\n",
    "print('----------------------------')\n",
    "for i in range(len(sessions)):\n",
    "    print('%s: %s' % (i, sessions[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the session file to analyze\n",
    "file_idx = int(input('Enter the session number to analyze:'))\n",
    "fileName = sessions[file_idx].split('/')[-1]\n",
    "fType = fileName[0:3]\n",
    "\n",
    "# Print pID\n",
    "pID = fileName\n",
    "proj_path = '/Users/justincampbell/Library/CloudStorage/GoogleDrive-u0815766@gcloud.utah.edu/My Drive/Research Projects/BLAESUnits'\n",
    "save_path = os.path.join(proj_path, 'Results', pID)\n",
    "\n",
    "\n",
    "# Utah files\n",
    "if fType == 'UIC':\n",
    "    # Locate files\n",
    "    nev = os.path.join(data_path, sessions[file_idx], sessions[file_idx] + '.nev')\n",
    "    nsx = os.path.join(data_path, sessions[file_idx], sessions[file_idx] + '.ns6')\n",
    "        \n",
    "    # Load .nev and .nsx files\n",
    "    events, waveforms = loadNEV(nev)\n",
    "    cont_data, header = loadNSX(nsx)\n",
    "\n",
    "    # Adjust for Blackrock's convention of multiplying uV by 4\n",
    "    waveforms = waveforms / 4\n",
    "\n",
    "    # Get chan IDs and chan labels\n",
    "    chan_IDs = [header[i]['ElectrodeID'] for i in range(len(header))]\n",
    "    chan_labels = [header[i]['ElectrodeLabel'] for i in range(len(header))]\n",
    "\n",
    "    # Create a dictionary to map chan_IDs to chan_labels\n",
    "    chan_map = {}\n",
    "    for idx, val in enumerate(chan_IDs):\n",
    "        chan_map[val] = chan_labels[idx] # adjust for Python indexing\n",
    "\n",
    "    # Map electrode labels to channel numbers\n",
    "    events['Channel'] = events['Channel'].map(chan_map)\n",
    "    \n",
    "    # Get sync & PD channels\n",
    "    sync = cont_data[chan_labels.index('Sync'),:]\n",
    "    PD = cont_data[chan_labels.index('PD'),:]\n",
    "    \n",
    "    # # Delete cont_data to save memory\n",
    "    # del cont_data\n",
    "\n",
    "\n",
    "# WashU files\n",
    "elif fType == 'BJH':\n",
    "    # Locate files\n",
    "    sorted_mat = os.path.join(data_path, sessions[file_idx], sessions[file_idx] + '_sorted.mat')\n",
    "    trigger_mat = os.path.join(data_path, sessions[file_idx], 'Stim_trigger.mat')\n",
    "    raw_mat = os.path.join(data_path, sessions[file_idx], sessions[file_idx] + '_raw.mat')\n",
    "    \n",
    "    # Load sorted.mat and raw.mat files\n",
    "    mat_dict, chan_labels = loadSortedMat(sorted_mat)\n",
    "    sync = loadmat(trigger_mat, simplify_cells = True)['stimTrigger']\n",
    "    try:\n",
    "        cont_data = loadmat(raw_mat, simplify_cells = True)['signals']\n",
    "    except:\n",
    "        cont_data = mat73.loadmat(raw_mat)['signals'] # if saved with v7.3 compression\n",
    "    \n",
    "    # Get events and waveforms\n",
    "    events = []\n",
    "    waveforms = []\n",
    "    for k, v in mat_dict.items():\n",
    "        df = pd.DataFrame(mat_dict[k])\n",
    "        df['Channel'] = [chan_labels[int(df[0][i]-1)] for i in range(df.shape[0])]\n",
    "        df['Unit'] = [int(df[1][i]) for i in range(df.shape[0])]\n",
    "        df['TimeStamps'] = [int(df[2][i]*fs) for i in range(df.shape[0])]\n",
    "        events.append(df[['TimeStamps', 'Unit', 'Channel']])\n",
    "        waveforms.append(df.iloc[:, 6:-3])\n",
    "    events = pd.concat(events)\n",
    "    waveforms = np.array(pd.concat(waveforms))\n",
    "    \n",
    "    # Adjust uV\n",
    "    waveforms = waveforms / 4\n",
    "    \n",
    "else:\n",
    "    print('Filetype not recognized.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get micro lead (bundle) labels\n",
    "micro_chans = [x for x in chan_labels if x.startswith('m')]\n",
    "active_micro_chans = [x for x in events['Channel'].unique().tolist()]\n",
    "micro_leads = set([x[:-1] for x in micro_chans])\n",
    "active_micro_leads = set([x[:-1] for x in active_micro_chans])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Epochs\n",
    "Epochs are created using the `sync` and `PD` channels within the recording to define the stim/no-stim trials, respectively. Because of the way in which the data were recorded, as well as noise/artifact present in the `sync`/`PD` channels, this section entails a considerable amount of manual tuning to ensure epochs are correctly aligned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Load Trial Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_info = pd.read_csv(os.path.join(proj_path, 'TrialInfo', pID + '_TrialInfo.csv'))\n",
    "trial_info[['Onset_SC', 'Onset_PD']] = trial_info[['Onset_SC', 'Onset_PD']].apply(lambda x: x * 15) # convert from 2->30 kHz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Detect Sync Channel Onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSyncOnsets(sync):\n",
    "    '''\n",
    "    Detect sync pulses within the 30 kHz recording.\n",
    "    \n",
    "    Arguments:\n",
    "        sync (np.ndarray): sync channel data\n",
    "        \n",
    "    Returns:\n",
    "        stim_onsets (np.ndarray): array of stim_onset indices\n",
    "    '''\n",
    "\n",
    "    # Define threshold for sync channel\n",
    "    if fType == 'UIC':\n",
    "        thresh = 15000\n",
    "    elif fType == 'BJH':\n",
    "        thresh = 15000\n",
    "\n",
    "    # Find stim pulses using the following criteria: \n",
    "    # (1) sync > threshold, (2) transitioning from below to above threshold, (3) > 5s since last crossing\n",
    "    stim_onsets = []\n",
    "    syncBool = (sync > thresh).astype(int)\n",
    "    counter = 0\n",
    "    for i in range(len(syncBool)-1):\n",
    "        if (syncBool[i] == 1) & (syncBool[-i] == 0) & (counter > (fs*5)):\n",
    "            stim_onsets.append(i)\n",
    "            counter = 0\n",
    "        counter += 1\n",
    "    \n",
    "    return stim_onsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect sync (stim) pulses\n",
    "sync_onsets = getSyncOnsets(sync)\n",
    "\n",
    "# Add sync_onsets to trial_info\n",
    "special_cases = ['BJH02503', 'BJH02703', 'BJH04502']\n",
    "stim_trial_counter = 0\n",
    "if pID not in special_cases:\n",
    "    for i in range(trial_info.shape[0]):\n",
    "        if trial_info.loc[i, 'Condition'] == 1:\n",
    "            trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter]\n",
    "            stim_trial_counter += 1\n",
    "        else:\n",
    "            trial_info.loc[i, 'Onset_Sync'] = np.nan\n",
    "else:\n",
    "    if pID == 'BJH02503':\n",
    "        # Sync pulses not present for 1st few trials, but LFP shows stim\n",
    "        for i in range(trial_info.shape[0]):\n",
    "            if trial_info.loc[i, 'Condition'] == 1:\n",
    "                if stim_trial_counter < 6:\n",
    "                    trial_info.loc[i, 'Onset_Sync'] = trial_info.loc[i, 'Onset_SC'] - 18000 # empirical correction\n",
    "                else:\n",
    "                    trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter-6]\n",
    "                stim_trial_counter += 1\n",
    "            else:\n",
    "                trial_info.loc[i, 'Onset_Sync'] = np.nan\n",
    "                \n",
    "    if pID == 'BJH02703':\n",
    "        # Many sync pulses not present\n",
    "        fixed = False\n",
    "        for i in range(trial_info.shape[0]):\n",
    "            if trial_info.loc[i, 'Condition'] == 1:\n",
    "                if (stim_trial_counter >= 27) and (stim_trial_counter < 40):\n",
    "                    trial_info.loc[i, 'Onset_Sync'] = trial_info.loc[i, 'Onset_SC'] - 10000 # empirical correction\n",
    "                    fixed = True\n",
    "                else:\n",
    "                    if not fixed:\n",
    "                        trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter+1]\n",
    "                    else:\n",
    "                        trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter-13]\n",
    "                stim_trial_counter += 1\n",
    "                \n",
    "    elif pID == 'BJH04502':\n",
    "        # Missed detection of one pulse\n",
    "        fixed = False\n",
    "        for i in range(trial_info.shape[0]):\n",
    "            if trial_info.loc[i, 'Condition'] == 1:\n",
    "                if stim_trial_counter == 33:\n",
    "                    trial_info.loc[i, 'Onset_Sync'] = trial_info.loc[i, 'Onset_SC'] - 13000 # empirical correction\n",
    "                    fixed = True\n",
    "                else:\n",
    "                    if not fixed:\n",
    "                        trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter]\n",
    "                    else:\n",
    "                        trial_info.loc[i, 'Onset_Sync'] = sync_onsets[stim_trial_counter-1]\n",
    "                stim_trial_counter += 1\n",
    "            else:\n",
    "                trial_info.loc[i, 'Onset_Sync'] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Define Stim/No-Stim Epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 Align Blackrock to BCI2000 w/ PD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utah data does not have 1:1 mapping to timing in BCI2000, thus this step includes an empirical re-alignment to a common feature\n",
    "# present in both datasets, the photodiode (PD) signal. \n",
    "\n",
    "# Get PD onsets from trial_info\n",
    "PD_onsets = trial_info['Onset_PD'].values\n",
    "\n",
    "# Find peaks in PD channel (30 kHz); some trial onsets are +PD vs. -PD spike (depending on experiment)\n",
    "if pID in ['UIC20230201']:\n",
    "    peaks_30k, _ = find_peaks(PD*-1, height = 500, distance = 150000)\n",
    "else:\n",
    "    if fType == 'UIC':\n",
    "        peaks_30k, _ = find_peaks(PD, height = 500, distance = 150000)\n",
    "\n",
    "# Manual tuning (empirical correction from visual inspection)\n",
    "if pID == 'UIC20221301':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(2,47), np.arange(48, 93), np.arange(94, 139), np.arange(140, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20221501':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(2,47), np.arange(48, 93), np.arange(94, 139), np.arange(140, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20221701':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(3,48), np.arange(49, 94), np.arange(98, 143), np.arange(144, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "\n",
    "elif pID == 'UIC20230201':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(2,82), np.arange(83, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20230601':\n",
    "    matched_peaks = np.empty(len(trial_info)) * np.nan # initialize as nans; (missing PD for 17 trials)\n",
    "    keep_peaks = np.concatenate((np.arange(3,67), np.arange(68, 148), np.arange(150, 230), np.arange(231, len(peaks_30k)))) # realign\n",
    "    matched_peaks[16:] = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20230701':\n",
    "    peaks_30k = peaks_30k[3:] # remove spurious PDs\n",
    "    keep_peaks = np.concatenate((np.arange(0,12), np.arange(13,81), np.arange(82,162), np.arange(163,243), np.arange(244,324))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20230801':\n",
    "    peaks_30k = peaks_30k[4:] # remove spurious PDs\n",
    "    keep_peaks = np.concatenate((np.arange(0,80), np.arange(81,161), np.arange(162,242), np.arange(243,323))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20231101':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(3,83), np.arange(84, 164), np.arange(165, 245), np.arange(247, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20231401':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(2,122), np.arange(123, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "elif pID == 'UIC20240101':\n",
    "    matched_peaks = peaks_30k\n",
    "    keep_peaks = np.concatenate((np.arange(3,123), np.arange(124, len(peaks_30k)))) # realign\n",
    "    matched_peaks = peaks_30k[keep_peaks]\n",
    "    \n",
    "# O/S cohort (must remove scrambled images)\n",
    "if pID in ['UIC20221301', 'UIC20221501', 'UIC20221701']:\n",
    "    PD_block_idxs = [np.arange(0, 40), np.arange(40, 80), np.arange(80, 120), np.arange(120, 160)]\n",
    "    peaks_block_idxs = [np.arange(0, 45), np.arange(45, 90), np.arange(90, 135), np.arange(135, 180)]\n",
    "    \n",
    "    # Get peaks and onsets for each block\n",
    "    matched_peaks_noscramb = []\n",
    "    for i in range(len(PD_block_idxs)):\n",
    "        peaks_block = matched_peaks[peaks_block_idxs[i]]\n",
    "        PD_onsets_block = PD_onsets[PD_block_idxs[i]]\n",
    "\n",
    "        # Adjust PD onsets to align with 1st peak in block (\"blocks\" are separate .dat files stitched together)\n",
    "        PD_onsets_adj = PD_onsets_block + (peaks_block[0] - PD_onsets_block[0])\n",
    "\n",
    "        # Get the values in peaks closest to PD_onsets_adj\n",
    "        peaks_adj_block = []\n",
    "        for i in range(len(PD_onsets_adj)):\n",
    "            peaks_adj_block.append(matched_peaks[np.argmin(np.abs(matched_peaks - PD_onsets_adj[i]))])\n",
    "        matched_peaks_noscramb.append(peaks_adj_block)\n",
    "        \n",
    "    matched_peaks = np.array([item for sublist in matched_peaks_noscramb for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite Utah trial_info PD onsets using matched_peaks\n",
    "if fType == 'UIC':\n",
    "    trial_info['Onset_PD'] = matched_peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Use Sync & PD to Define Stim/No-Stim Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_epochs = []\n",
    "nostim_epochs = []\n",
    "\n",
    "for i in range(trial_info.shape[0]):\n",
    "    # stim trial\n",
    "    if trial_info.loc[i, 'Condition'] == 1:\n",
    "        stim_epochs.append(np.arange(trial_info.loc[i, 'Onset_Sync'] - (4 * fs), trial_info.loc[i, 'Onset_Sync'] + (2*fs)))\n",
    "    # no stim trial\n",
    "    else:\n",
    "        if np.isnan(trial_info['Onset_PD'][i]):\n",
    "            nostim_epochs.append(np.zeros(fs*6)) # populate empty epochs with zeros (cannot use np.nan in pd.DataFrame)\n",
    "        else:\n",
    "            nostim_epochs.append(np.arange(trial_info.loc[i, 'Onset_PD'] - (4 * fs), trial_info.loc[i, 'Onset_PD'] + (2*fs)))\n",
    "        \n",
    "stim_epochs = pd.DataFrame(stim_epochs).astype(int).T\n",
    "nostim_epochs = pd.DataFrame(nostim_epochs).astype(int).T\n",
    "\n",
    "stim_epochs.to_csv(os.path.join(save_path, 'StimEpochs.csv'))\n",
    "nostim_epochs.to_csv(os.path.join(save_path, 'NoStimEpochs.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Epoch 30 kHz Continuous Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stim_epochs_30k = []\n",
    "nostim_epochs_30k = []\n",
    "\n",
    "for i in range(stim_epochs.shape[1]):\n",
    "    stim_epochs_30k.append(cont_data[:, stim_epochs[i].values])\n",
    "    \n",
    "for i in range(nostim_epochs.shape[1]):\n",
    "    nostim_epochs_30k.append(cont_data[:, nostim_epochs[i].values])\n",
    "    \n",
    "stim_epochs_30k = np.array(stim_epochs_30k)\n",
    "nostim_epochs_30k = np.array(nostim_epochs_30k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter epochs to just include micro channels\n",
    "stim_epochs_30k_micro = []\n",
    "for i in range(stim_epochs_30k.shape[0]):\n",
    "    stim_epochs_30k_micro.append(stim_epochs_30k[i, [chan_labels.index(x) for x in micro_chans]])\n",
    "stim_epochs_30k_micro = np.array(stim_epochs_30k_micro)\n",
    "\n",
    "nostim_epochs_30k_micro = []\n",
    "for i in range(nostim_epochs_30k.shape[0]):\n",
    "    nostim_epochs_30k_micro.append(nostim_epochs_30k[i, [chan_labels.index(x) for x in micro_chans]])\n",
    "nostim_epochs_30k_micro = np.array(nostim_epochs_30k_micro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate into pre trial (1st second) and post trial (last second)\n",
    "stim_epochs_30k_pre = stim_epochs_30k_micro[:, :, :4*fs]\n",
    "stim_epochs_30k_post = stim_epochs_30k_micro[:, :, -fs:]\n",
    "nostim_epochs_30k_pre = nostim_epochs_30k_micro[:, :, :fs]\n",
    "nostim_epochs_30k_post = nostim_epochs_30k_micro[:, :, -fs:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MUA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractMUA(data):\n",
    "    \"\"\"\n",
    "    Detects multiunit activity (MUA) in neural recordings using RMS thresholding.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy array of shape [channels x samples], sampled at 30 kHz\n",
    "\n",
    "    Returns:\n",
    "    - mua: filtered MUA signals\n",
    "    - timestamps: list of spike timestamps (in seconds) for each channel\n",
    "    - thresh: last computed threshold value\n",
    "    \"\"\"\n",
    "    \n",
    "    def find_inflections(signal, kind='minima'):\n",
    "        \"\"\"Find local minima of the signal.\"\"\"\n",
    "        if kind == 'minima':\n",
    "            # Invert signal to find minima using find_peaks\n",
    "            peaks, _ = find_peaks(-signal)\n",
    "            return peaks\n",
    "        else:\n",
    "            raise ValueError(\"Only 'minima' kind is supported.\")\n",
    "    \n",
    "    Fs = 30000  # Sampling rate\n",
    "    band = [300, 3000]  # Bandpass range in Hz\n",
    "    num_chans = data.shape[0]\n",
    "    stim_win = [4, 5]\n",
    "\n",
    "    # Design FIR filter\n",
    "    b = firwin(numtaps=97, cutoff=[band[0], band[1]], pass_zero=False, fs=Fs)\n",
    "    a = 1  # FIR filters have only b coefficients\n",
    "\n",
    "    mua = np.zeros_like(data, dtype=float)\n",
    "    timestamps = []\n",
    "\n",
    "    for ch in range(num_chans):\n",
    "        filtered = filtfilt(b, a, data[ch, :].astype(float))\n",
    "        filtered -= np.mean(filtered)\n",
    "        filtered_pre = filtered[:Fs]  # Pre-trial segment\n",
    "        mua[ch, :] = filtered\n",
    "        thresh = -3.5 * np.sqrt(np.mean(filtered_pre**2))  # RMS threshold\n",
    "        peaks = find_inflections(filtered, kind='minima')\n",
    "        spiketimes = peaks[filtered[peaks] < thresh]\n",
    "        timestamps.append(spiketimes)\n",
    "        \n",
    "    # Count spikes in pre-/post- epochs\n",
    "    pre_counts = np.zeros(num_chans)\n",
    "    post_counts = np.zeros(num_chans)\n",
    "    for ch in range(num_chans):\n",
    "        pre_spikes = timestamps[ch][timestamps[ch] <= 1 * Fs]\n",
    "        post_spikes = timestamps[ch][timestamps[ch] >= 5 * Fs]\n",
    "        pre_counts[ch] = len(pre_spikes)\n",
    "        post_counts[ch] = len(post_spikes)\n",
    "        \n",
    "    # Organize into a DataFrame\n",
    "    counts_df = pd.DataFrame({\n",
    "        'Chan': [chan_labels[ch] for ch in range(num_chans)],\n",
    "        'Pre_Spikes': pre_counts,\n",
    "        'Post_Spikes': post_counts\n",
    "    })\n",
    "        \n",
    "    # Remove timestamps that are inside the stim window\n",
    "    for ch in range(num_chans):\n",
    "        valid_spikes = []\n",
    "        for ts in timestamps[ch]:\n",
    "            if ts < stim_win[0] * Fs or ts > stim_win[1] * Fs:\n",
    "                valid_spikes.append(ts)\n",
    "        timestamps[ch] = np.array(valid_spikes)\n",
    "\n",
    "    return mua, timestamps, thresh, counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials_stim = stim_epochs_30k_pre.shape[0]\n",
    "n_trials_nostim = nostim_epochs_30k_pre.shape[0]\n",
    "\n",
    "stim_dfs = []\n",
    "for i in range(n_trials_stim):\n",
    "    _, _, _, counts_df = extractMUA(stim_epochs_30k_micro[i, :, :])\n",
    "    counts_df['Trial'] = i + 1\n",
    "    stim_dfs.append(counts_df)\n",
    "stim_dfs = pd.concat(stim_dfs, ignore_index=True)\n",
    "stim_dfs['Condition'] = 'Stim'\n",
    "stim_dfs['pID'] = pID\n",
    "stim_dfs = stim_dfs.reset_index(drop=True)\n",
    "\n",
    "nostim_dfs = []\n",
    "for i in range(n_trials_nostim):\n",
    "    _, _, _, counts_df = extractMUA(nostim_epochs_30k_micro[i, :, :])\n",
    "    counts_df['Trial'] = i + 1\n",
    "    nostim_dfs.append(counts_df)\n",
    "nostim_dfs = pd.concat(nostim_dfs, ignore_index=True)\n",
    "nostim_dfs['Condition'] = 'NoStim'\n",
    "nostim_dfs['pID'] = pID\n",
    "nostim_dfs = nostim_dfs.reset_index(drop=True)\n",
    "\n",
    "# Combine stim and nostim DataFrames\n",
    "mua_counts = pd.concat([stim_dfs, nostim_dfs], ignore_index=True)\n",
    "mua_counts = mua_counts.reset_index(drop=True)\n",
    "\n",
    "# Export MUA counts to CSV\n",
    "mua_counts.to_csv(os.path.join(save_path, 'MUACounts.csv'))\n",
    "mua_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example MUA for Supplemental Figure 8\n",
    "\n",
    "# # UIC20230601 (7), mRHCA4 (18), trial 127 \n",
    "# test_idx = 19\n",
    "# trial_idx = 126\n",
    "# Fs = 30000\n",
    "\n",
    "# mua, timestamps, thresh, counts_df = extractMUA(stim_epochs_30k_micro[trial_idx, :, :])\n",
    "\n",
    "# # plot MUA\n",
    "# fig, axes = plt.subplots(2, 1, figsize = (8, 2), gridspec_kw={'height_ratios': [3, 1]}, sharex = True)\n",
    "# axes[1].set_position([0.125, 0.3, 0.775, 0.15])  # [left, bottom, width, height]\n",
    "\n",
    "\n",
    "# axes[0].plot(mua[test_idx, :], label='Filtered MUA', lw = 0.25, color = '#545352')\n",
    "# sns.rugplot(timestamps[test_idx], ax=axes[1], color='#da1b61', height=0.5, lw = 1)\n",
    "# axes[0].scatter(timestamps[test_idx], mua[test_idx, (timestamps[test_idx])], color='#da1b61', s=0.005, label='Threshold Crossing', zorder = 10)\n",
    "\n",
    "\n",
    "\n",
    "# axes[0].axvspan(xmin = 4*Fs, xmax = 5*Fs, color='grey', alpha = 0.15, zorder = 10)\n",
    "# axes[0].axvline(Fs, color = 'k', linestyle = '--', lw = 0.5)\n",
    "# axes[0].axvline(4*Fs, color = 'k', linestyle = '--', lw = 0.5)\n",
    "# axes[0].axvline(5*Fs, color = 'k', linestyle = '--', lw = 0.5)\n",
    "# # axes[0].axhline(thresh, color = '#da1b61', linestyle = '-', lw = 0.5, label='Threshold', zorder = -1) # subtract 8 to account for lw\n",
    "\n",
    "# for ax in axes:\n",
    "#     ax.tick_params(axis='both', which='both', bottom=False, left=False, labelbottom=False, labelleft=False)\n",
    "#     sns.despine(ax=ax, top=True, right=True, left=True, bottom=True)\n",
    "#     ax.set_xlim(-1000, (6 * Fs)+1000)  # Set x-axis limits to 6 seconds (180000 samples)\n",
    "# # axes[0].set_ylim(-400, 400)\n",
    "# # axes[0].text(-0.045, 0.42, '~100 $\\mu$V', fontsize = 'x-small', rotation = 90, transform = axes[0].transAxes)\n",
    "# # axes[1].text(0.07, 0.175, '1 s', fontsize = 'x-small', transform = axes[0].transAxes)\n",
    "# axes[0].text(0.05, 1.1, 'Pre-ISI', fontsize = 'small', transform = axes[0].transAxes)\n",
    "# axes[0].text(0.325, 1.1, 'Image', fontsize = 'small', transform = axes[0].transAxes)\n",
    "# axes[0].text(0.68, 1.1, 'Stim | No-Stim', fontsize = 'small', transform = axes[0].transAxes)\n",
    "# axes[0].text(0.88, 1.1, 'Post-ISI', fontsize = 'small', transform = axes[0].transAxes)\n",
    "\n",
    "# plt.savefig(os.path.join('/Users/justincampbell/Library/CloudStorage/GoogleDrive-u0815766@gcloud.utah.edu/My Drive/Research Projects/BLAESUnits/Presentations/Methods Figures/MUAEx.pdf'), dpi=1000, bbox_inches='tight')\n",
    "# plt.savefig(os.path.join('/Users/justincampbell/Library/CloudStorage/GoogleDrive-u0815766@gcloud.utah.edu/My Drive/Research Projects/BLAESUnits/Presentations/Methods Figures/MUAEx.png'), dpi=1000, bbox_inches='tight')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
